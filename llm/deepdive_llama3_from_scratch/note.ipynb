{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664c0203-a789-4634-86b5-cff7c4b33d00",
   "metadata": {},
   "source": [
    "https://github.com/therealoliver/Deepdive-llama3-from-scratch/blob/main/README_zh.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cab5328-061f-4359-8516-bdedcf83b47d",
   "metadata": {},
   "source": [
    "**环境准备**\n",
    "- 下载模型：modelscope download --model LLM-Research/Llama-3.2-1B-Instruct --local_dir ./Llama-3.2-1B-Instruct\n",
    "- 下载pip包：pip install sentencepiece tiktoken torch blobfile matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdad82d0-042b-4005-a9b8-1f69d478b463",
   "metadata": {},
   "source": [
    "# 加载模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3636c85-cb79-405a-9313-e436c70a7f3f",
   "metadata": {},
   "source": [
    "## 加载分词器tokenizer\n",
    "tokenizer作用（原始文本 <-> Token）\n",
    "- 文本拆分：Hello world -> [\"hello\", \"world\"]\n",
    "- 映射：Hello -> 15496\n",
    "- 处理特殊标记：[CLS]表示句子开头\n",
    "- 反向映射：15496 -> Hello\n",
    "\n",
    "加载基于BPE的tokenizer\n",
    "- 加载模型字典（无特殊token）\n",
    "- 加入特殊token到字典：手动定义或基于模型现有的（tokenizer_config.json）\n",
    "- 定义文本分割规则：粗分割（基于正则）、细分割（基于BPE）\n",
    "- 创建tokenizer：基于tiktoken创建文本编解码对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8bae14b-b46e-4e67-a78b-25f58572633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create tokenizer successed!\n",
      "原始字符串: Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.\n",
      "正则分割结果: ['Hello', ' world', '!', ' It', \"'s\", ' a', ' test', '.', ' 这是一个测试', '.', ' alongwords', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']\n",
      "tokenizer分割结果: ['Hello', ' world', '!', ' It', \"'s\", ' a', ' test', '.', ' 这', '是一个', '测试', '.', ' along', 'words', '.', ' a', ' long', ' words', '.', ' ', '123', ' ', '456', ' ', '789', '.']\n",
      "tokenizer分割结果id: [('Hello', 9906), (' world', 1917), ('!', 0), (' It', 1102), (\"'s\", 596), (' a', 264), (' test', 1296), ('.', 13), (' 这', 122255), ('是一个', 122503), ('测试', 82805), ('.', 13), (' along', 3235), ('words', 5880), ('.', 13), (' a', 264), (' long', 1317), (' words', 4339), ('.', 13), (' ', 220), ('123', 4513), (' ', 220), ('456', 10961), (' ', 220), ('789', 16474), ('.', 13)]\n"
     ]
    }
   ],
   "source": [
    "# 加载基于BPE的tokenizer\n",
    "\n",
    "# 导入相关库\n",
    "from pathlib import Path  # 用于从文件路径中获取文件名/模型名\n",
    "import tiktoken  # openai开发的开源库，用于文本编解码（文本和tokenid的相互转换）\n",
    "from tiktoken.load import load_tiktoken_bpe  # 加载BPE模型\n",
    "import torch  # 用于搭建模型和矩阵计算\n",
    "import json  # 用于加载配置文件\n",
    "import matplotlib.pyplot as plt  # 用于绘图\n",
    "\n",
    "tokenizer_path = \"/data/models/Llama-3.2-1B-Instruct/original/tokenizer.model\"  # 分词器模型的路径\n",
    "\n",
    "# 常规词典外的特殊token\n",
    "# 在\"Meta-Llama-3-8B/\"路径下的'tokenizer.json'和'tokenizer_config.json'的added_tokens字段下都有这些特殊token\n",
    "special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",  # 保留了从0到250的特殊token，出于功能扩展性、任务兼容性和未来安全性的考虑\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",  # 头部信息的开始，用于标记包裹结构化数据的头部信息，如元数据\n",
    "            \"<|end_header_id|>\",  # 头部信息的结束\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn，多轮对话里标记当前轮次对话的结束\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "\n",
    "# 加载BPE模型（实际是一个字典）\n",
    "# 一个字典，子词(bytes类型，用utf-8解码)-rank(id)对，128000词，不包含上面的256个特殊token（所以模型的总词典大小是128256）\n",
    "# 其中rank值是从0递增的序列，用于决定子词单元合并的优先顺序，优先级越高的会优先合并，因此这里的名字是mergeable ranks而非BPE或字典等类似的名字\n",
    "# 没把特殊token加到字典里应该是出于灵活性考虑，便于面对不同模型架构或任务有不同特殊token时添加特定的token，而且保持字典大小不变\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "\n",
    "# 创建一个文本编解码器对象\n",
    "# 其中的pat_str大致分为三个类型：带缩写的单词 & 单词、中文片段、1-3位的数字 & 其他特殊字符\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,  # 编码器名称，便于调试和日志记录使用的不同的编码器\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",  # 用于初步的粗分割文本为token序列的正则表达式\n",
    "    mergeable_ranks=mergeable_ranks,  # 传入加载的BPE模型\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},  # 添加特殊token-id对的字典\n",
    ")\n",
    "\n",
    "# 测试是否创建成功，即编解码器是否能正确运行\n",
    "print(tokenizer.decode(tokenizer.encode(\"create tokenizer successed!\")))\n",
    "\n",
    "\n",
    "# 下面是一个案例测试，来测试pat_str粗分割和tokenizer细分割的效果和区别\n",
    "# pat_str的正则只是提供了一个初步的分割，一些长句子或中文等不会分割，会在tokenizer中进一步基于BPE算法进行细化分割\n",
    "import regex  # 由于pat_str中用到了Unicode的一些语法，如\\p{L}，所以不能用re库\n",
    "\n",
    "## 创建正则\n",
    "pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n",
    "pattern = regex.compile(pat_str)\n",
    "\n",
    "## 文本切分\n",
    "text = \"Hello world! It's a test. 这是一个测试. alongwords. a long words. 123 456 789.\"  # 测试文本\n",
    "re_tokens = pattern.findall(text)  # 使用正则表达式分割字符串\n",
    "merge_tokens_id = tokenizer.encode(text)  # 使用tokenizer分割字符串\n",
    "merge_tokens = [tokenizer.decode([i]) for i in merge_tokens_id]  # 将tokenizer分割结果的id序列转换为实际的子词序列\n",
    "\n",
    "## 结果输出\n",
    "print(\"原始字符串:\", text)\n",
    "print(\"正则分割结果:\", re_tokens)\n",
    "print(\"tokenizer分割结果:\", merge_tokens)\n",
    "print(\"tokenizer分割结果id:\", list(zip(merge_tokens, merge_tokens_id)))\n",
    "\n",
    "## 从结果将会看到所有单词的前缀空格都被保留了下来，而非单独一个空格token或将其删除，有利于模型正确理解单词间的边界信息，如例子中的alongwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5f6700-edd6-40f9-b4bc-9a08e1bfe6a6",
   "metadata": {},
   "source": [
    "## 读取读取模型文件和配置文件\n",
    "配置文件params.json\n",
    "- dim：隐藏层维度\n",
    "- n_layers：模型层数\n",
    "- n_heads：多头注意力的头数，所谓的多头即同时使用了多个独立的注意力机制，以捕捉输入数据的不同特征或信息\n",
    "- n_kv_heads：键值注意力的头数，用于分组查询注意力GQA，即键值注意力有8个头，而查询有n_heads=32个头，每4个查询头会共享一组键值对\n",
    "- vocab_size：词汇表大小，128000个普通token，256个特殊token\n",
    "- multiple_of：隐藏层维度的倍数约束，即限制模型隐藏层维数应为`multiple_of`的倍数，从而优化计算效率\n",
    "- ffn_dim_multiplier：前馈网络层的隐藏层维度乘数，用于计算FFN隐藏层维度，计算过程可见对应\n",
    "- norm_eps：层归一化计算中在分母里加的常量，防止除零，保证数值稳定性\n",
    "- rope_theta：旋转位置编码RoPE中的基础频率缩放因子，控制位置编码的周期性和分辨率，从而影响模型对不同长度序列和位置关系的捕捉能力\n",
    "\n",
    "attention内部计算流程\n",
    "```python\n",
    "input(L, 4096) -> query_proj(L, 128, 32) -> query_proj(L, 128, 32) -> query_proj(L, 128, 32)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27dd94b-7424-4d53-b339-96f1b2d0cbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2754014/300294595.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"/data/models/Llama-3.2-1B-Instruct/original/consolidated.00.pth\")\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/models/Llama-3.2-1B-Instruct/original/consolidated.00.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 加载模型，一个网络层名称-tensor类型参数的字典\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/data/models/Llama-3.2-1B-Instruct/original/consolidated.00.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 输出前20层网络名，验证是否正确加载\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mlist\u001b[39m(model\u001b[38;5;241m.\u001b[39mkeys())[:\u001b[38;5;241m20\u001b[39m], indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/huangzhiwen/lib/python3.10/site-packages/torch/serialization.py:1065\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m   1063\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 1065\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1067\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/huangzhiwen/lib/python3.10/site-packages/torch/serialization.py:468\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 468\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    469\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    470\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/huangzhiwen/lib/python3.10/site-packages/torch/serialization.py:449\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 449\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/models/Llama-3.2-1B-Instruct/original/consolidated.00.pth'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 加载模型，一个网络层名称-tensor类型参数的字典\n",
    "model = torch.load(\"/data/models/Llama-3.2-1B-Instruct/original/consolidated.00.pth\")\n",
    "\n",
    "# 输出前20层网络名，验证是否正确加载\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))\n",
    "\n",
    "# 加载配置文件，每个配置的具体含义见下节\n",
    "with open(\"Meta-Llama-3-8B/original/params.json\", \"r\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6efc20-714e-40a1-b444-7400788cac6a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb51dc63-b0a3-4fae-8e3c-b61b46154049",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39e44de-87b5-4713-9ed2-fc39576d9b7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec4f99ef-1683-4c16-8691-20d03f71c8ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f477a3e0-0e5c-4d1c-b652-ddcdf3878c96",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25835249-190d-4fa6-89f1-040d333806ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b54caa7d-7a16-4f4f-a8da-df058c7afa02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
